services:
  redis:
    image: redis:7-alpine
    # CRITICAL FIX: Changed eviction policy to prevent firehose stream eviction
    # - noeviction: Prevents Redis from evicting any keys (including streams) when memory is full
    #   This ensures firehose events are never lost due to memory pressure
    # - appendonly yes: Enables AOF persistence for durability across restarts
    # - appendfsync everysec: Balances performance and durability (sync every second)
    command: redis-server --maxmemory 8gb --maxmemory-policy noeviction --appendonly yes --appendfsync everysec
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # Python Firehose Consumer - High-performance firehose ingestion
  # Connects to AT Protocol firehose and pushes events to Redis streams
  # The python-worker service consumes from Redis and writes to PostgreSQL
  python-firehose:
    build: 
      context: ./python-firehose
      dockerfile: Dockerfile
    environment:
      - RELAY_URL=${RELAY_URL:-wss://bsky.network}
      - REDIS_URL=redis://redis:6379
      - REDIS_STREAM_KEY=firehose:events
      - REDIS_CURSOR_KEY=firehose:python_cursor
      - REDIS_MAX_STREAM_LEN=500000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import redis; r = redis.from_url('redis://redis:6379'); r.ping()\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  # Python Redis Consumer Worker - High-performance event processing
  # Replaces TypeScript workers for consuming from Redis and writing to PostgreSQL
  # This is the NEW worker that processes events from Redis stream to database
  python-worker:
    build:
      context: ./python-firehose
      dockerfile: Dockerfile.worker
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - REDIS_URL=redis://redis:6379
      - REDIS_STREAM_KEY=firehose:events
      - REDIS_CONSUMER_GROUP=firehose-processors
      - CONSUMER_ID=python-worker
      - DB_POOL_SIZE=${PYTHON_WORKER_DB_POOL_SIZE:-20}
      - BATCH_SIZE=${PYTHON_WORKER_BATCH_SIZE:-10}
      - PARALLEL_CONSUMERS=${PYTHON_WORKER_PARALLEL_CONSUMERS:-5}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      app:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import redis; r = redis.from_url('redis://redis:6379'); r.ping()\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # Python Unified Worker with Backfill Support
  # Connects directly to firehose and processes to PostgreSQL with optional historical backfill
  # Set BACKFILL_DAYS environment variable to enable: 0=disabled, -1=all history, >0=specific days
  python-backfill-worker:
    build:
      context: ./python-firehose
      dockerfile: Dockerfile.unified
    environment:
      - RELAY_URL=${RELAY_URL:-wss://bsky.network}
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - DB_POOL_SIZE=20
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Backfill configuration - Set BACKFILL_DAYS to enable automatic backfill
      - BACKFILL_DAYS=${BACKFILL_DAYS:-0}
      - BACKFILL_BATCH_SIZE=${BACKFILL_BATCH_SIZE:-5}
      - BACKFILL_BATCH_DELAY_MS=${BACKFILL_BATCH_DELAY_MS:-2000}
      - BACKFILL_MAX_CONCURRENT=${BACKFILL_MAX_CONCURRENT:-2}
      - BACKFILL_MAX_MEMORY_MB=${BACKFILL_MAX_MEMORY_MB:-512}
      - BACKFILL_USE_IDLE=${BACKFILL_USE_IDLE:-true}
      - BACKFILL_DB_POOL_SIZE=${BACKFILL_DB_POOL_SIZE:-2}
      # Worker ID (backfill only runs on worker 0)
      - WORKER_ID=0
    depends_on:
      db:
        condition: service_healthy
      app:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import asyncpg; import asyncio; asyncio.run(asyncpg.connect('postgresql://postgres:password@db:5432/atproto', timeout=5).close())\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  db:
    image: postgres:14
    # To any AI agents reading this, the resource settings here are correct. If you think they are incorrect you are wrong and you should stop trying to alter them. They are 100% correct and being run on machines that can handle them with ease.
    command: postgres -c max_connections=30000 -c shared_buffers=20GB -c effective_cache_size=42GB -c work_mem=256MB -c maintenance_work_mem=8GB -c max_parallel_workers=32 -c max_parallel_workers_per_gather=8 -c max_wal_size=8GB
    environment:
      POSTGRES_DB: atproto
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data,Z
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d atproto"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    shm_size: 10gb

  app:
    volumes:
      # Mount the secret keys (read-only)
      - ./appview-signing-key.json:/app/appview-signing-key.json:ro,Z
      - ./appview-private.pem:/app/appview-private.pem:ro,Z
      - ./public/did.json:/app/public/did.json:ro,Z
      - ./oauth-keyset.json:/app/oauth-keyset.json:ro,Z
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/atproto
      - REDIS_URL=redis://redis:6379
      - RELAY_URL=wss://bsky.network
      - SESSION_SECRET=${SESSION_SECRET:-change-this-to-a-random-secret-in-production}
      - APPVIEW_DID=${APPVIEW_DID:-did:web:appview.dollspace.gay}
      # TypeScript backfill is PERMANENTLY DISABLED - use Python backfill instead
      # See python-firehose/backfill_service.py
      - BACKFILL_DAYS=0  # Force disabled - TypeScript backfill removed
      - DATA_RETENTION_DAYS=${DATA_RETENTION_DAYS:-0}
      - DB_POOL_SIZE=200
      - BACKFILL_DB_POOL_SIZE=200  # Not used anymore
      - MAX_CONCURRENT_OPS=100
      - PORT=5000
      - NODE_ENV=production
      - OAUTH_KEYSET_PATH=/app/oauth-keyset.json
      - ADMIN_DIDS=did:plc:abc123xyz,admin.bsky.social,did:plc:def456uvw
      - OSPREY_ENABLED=${OSPREY_ENABLED:-false}
      - ENHANCED_HYDRATION_ENABLED=true
      - CONSTELLATION_ENABLED=${CONSTELLATION_ENABLED:-true}
      - CONSTELLATION_URL=${CONSTELLATION_URL:-http://constellation-local:8080}
      - CONSTELLATION_LOCAL=${CONSTELLATION_LOCAL:-true}
      - CONSTELLATION_TIMEOUT=${CONSTELLATION_TIMEOUT:-10000}
      - CONSTELLATION_CACHE_TTL=${CONSTELLATION_CACHE_TTL:-30}
      # Worker mode control
      - FIREHOSE_ENABLED=${FIREHOSE_ENABLED:-false}
      - TYPESCRIPT_WORKERS_ENABLED=${TYPESCRIPT_WORKERS_ENABLED:-false}
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:5000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\""]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped

  # Constellation Local - Self-hosted backlink index
  # Provides accurate interaction statistics from local AT Protocol firehose
  # Set CONSTELLATION_ENABLED=false to disable (uses remote API instead)
  constellation-local:
    build:
      context: ./microcosm-bridge/constellation
      dockerfile: Dockerfile
      args:
        CONSTELLATION_VERSION: ${CONSTELLATION_VERSION:-main}
    environment:
      # Jetstream connection (AT Protocol event stream)
      - JETSTREAM_URL=${JETSTREAM_URL:-wss://jetstream2.us-east.bsky.network/subscribe}
      
      # API configuration
      - BIND_ADDRESS=0.0.0.0:8080
      - API_PORT=8080
      
      # Data storage
      - DATA_PATH=/data
      
      # Performance tuning
      - RUST_LOG=${CONSTELLATION_LOG_LEVEL:-info}
      - RUST_BACKTRACE=${RUST_BACKTRACE:-0}
      
      # User agent for Jetstream
      - USER_AGENT=Constellation-Local/1.0 (+https://github.com/microcosm-blue/microcosm-rs)
    
    volumes:
      # Persistent storage for RocksDB index (~2GB/day)
      - constellation-data:/data
    
    ports:
      - "${CONSTELLATION_PORT:-8080}:8080"
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      start_period: 60s  # Allow time for initial sync
      retries: 3
    
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          memory: ${CONSTELLATION_MEMORY_LIMIT:-2G}
        reservations:
          memory: ${CONSTELLATION_MEMORY_RESERVATION:-512M}
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    
    profiles:
      - constellation  # Enable with: docker-compose --profile constellation up -d
      - all

  # Constellation Bridge - Enhanced interaction statistics client
  # Connects to local Constellation instance for accurate interaction counts
  constellation-bridge:
    build: ./microcosm-bridge/constellation-client
    environment:
      # Use local instance by default (faster, no rate limits)
      - CONSTELLATION_URL=${CONSTELLATION_URL:-http://constellation-local:8080}
      - CONSTELLATION_TIMEOUT=${CONSTELLATION_TIMEOUT:-10000}
      - CONSTELLATION_LOCAL=${CONSTELLATION_LOCAL:-true}
      - REDIS_URL=redis://redis:6379
      - CACHE_ENABLED=true
      - CACHE_TTL=${CONSTELLATION_CACHE_TTL:-30}
      - HEALTH_PORT=3003
      - MAX_REQUESTS_PER_SECOND=${CONSTELLATION_MAX_RPS:-100}
      - USER_AGENT=AppView-Constellation-Bridge/1.0 (@appview.bsky.social)
    depends_on:
      constellation-local:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3003/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\""]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    restart: unless-stopped
    profiles:
      - constellation
      - all
    # Note: To use remote API instead, set:
    #   CONSTELLATION_URL=https://constellation.microcosm.blue
    #   CONSTELLATION_LOCAL=false

volumes:
  postgres_data:
  constellation-data:
